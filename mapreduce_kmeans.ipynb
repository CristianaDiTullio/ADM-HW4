{"cells":[{"cell_type":"markdown","id":"b6becc38-05cd-49db-a9fb-9c6e68f20b83","metadata":{"id":"b6becc38-05cd-49db-a9fb-9c6e68f20b83"},"source":["## KMeans with MapReduce\n","\n","\n","Here the implementation of KMeans from scratch and using the MapReduce approach.\n","\n","---\n","\n","Definition of auxiliary functions:"]},{"cell_type":"code","execution_count":null,"id":"246c8be9-2feb-463f-be10-71c9eecb4c7d","metadata":{"tags":[],"id":"246c8be9-2feb-463f-be10-71c9eecb4c7d"},"outputs":[],"source":["#Import libraries\n","import numpy as np\n","\n","#Set global variables\n","MAX_ITERATIONS = 50\n","epsilon = 0.02\n","\n","#Function to remove first line\n","def remove_header(rdd):\n","    header = rdd.first()\n","    res = rdd.filter(lambda line: line != header)\n","    return res\n","\n","#Function to prepare the dataset\n","def preprocessing(rdd):\n","    rdd = remove_header(rdd) #remove first line (only contains column names)\n","    #Then:\n","    # - remove first column (only contains indexes)\n","    # - cast strings to floats\n","    # - cast list of floats to numpy array\n","    rdd = rdd.map(lambda row: row.split(\",\")[1:])\\\n","                .map(lambda row: [float(i) for i in row])\\\n","                .map(lambda row: np.array(row))    \n","    return rdd\n","\n","#Function to initialize centroids to random points in our dataset\n","def init_centroids(dataset, k):\n","    initial_centroids = dataset.takeSample(False, k)\n","    return initial_centroids\n","\n","#Function to compute euclidean distance between points\n","def eu_dist(v1, v2):\n","    dist = np.linalg.norm(v1 - v2)\n","    return dist\n","\n","#Function to be called by map\n","def mapper(point, centroids):\n","    distances = np.zeros(len(centroids))\n","    for i in range(len(centroids)):\n","        distances[i] = eu_dist(point, centroids[i])\n","    idx_min = np.argmin(distances)\n","    return idx_min\n","\n","#Function to calculate new centroids after the reduce step\n","#reduce_result = (i, (sum x_i, |S_i|)) for i = 0, ..., k - 1\n","def update_centroids(reduce_result, k):\n","    new_centroids = []\n","    result = reduce_result.take(k)\n","    for i in range(k):\n","        new_centroids.append(result[i][1][0] / result[i][1][1])\n","    return new_centroids\n","\n","#Function to broadcast the new centroids at every iteration\n","def broadcasting(current_centroids):\n","    centroids_broadcast = sc.broadcast(current_centroids)\n","    new_centroids = centroids_broadcast.value\n","    return new_centroids\n","\n","#Define our loss function to check how \"different\" new centroids are\n","#L(c1,...,ck) = ∑_i=1,..,n min_j||xi − cj||2\n","def error_function(old_centroids, new_centroids, k):\n","    distances = np.zeros(k)\n","    for i in range(k):\n","        distances[i] = eu_dist(old_centroids[i], new_centroids[i])   #distances = [d1, d2, d3, ..., dk]\n","    mean_distance = np.mean(distances)\n","    return mean_distance\n","\n","#Function to define the stopping condition of our program\n","#(looks at the global variables epsilon and MAX_ITERATIONS)\n","def stop_condition(mean_distance, n_iter):\n","    stop = False\n","    if (mean_distance <= epsilon or n_iter == MAX_ITERATIONS):\n","        stop = True\n","    return stop\n","\n","#Function to be applied on the final RDD before saving to file\n","def toCSVRow_components(data_row):\n","    temp = str(data_row[0]).strip('[').strip(']').strip().replace('  ', ' ') + ' ' + str(data_row[1])\n","    return temp.replace(' ', ',')\n","\n","#Function to be applied on the header of the RDD before saving to file\n","def toCSVRow_header(data_row):\n","    res = ','.join(e for e in data_row)\n","    return res"]},{"cell_type":"markdown","id":"489868c6-2daa-4ef6-b649-bed7f7234e54","metadata":{"id":"489868c6-2daa-4ef6-b649-bed7f7234e54"},"source":["---------\n","<br>\n","\n","Executing our KMeans:"]},{"cell_type":"code","execution_count":null,"id":"fb384ec5-9bac-48e8-baef-683b43f4eb5d","metadata":{"tags":[],"id":"fb384ec5-9bac-48e8-baef-683b43f4eb5d"},"outputs":[],"source":["#MAIN PROGRAM\n","\n","#Load data and preprocess it\n","components = sc.textFile('s3://admhw4/principal5.csv')\n","components_prep = preprocessing(components)\n","\n","#KMEANS-------------------------------------------------------------------------------------------------\n","#Initialization\n","k = 9                                                      #init number of clusters\n","initial_centroids = init_centroids(components_prep, k)     #init centroids to random points\n","centroids_broadcast = broadcasting(initial_centroids)      #broadcast initial centroids\n","n = 0                                                      #init number of iterations\n","stop = False                                               #init boolean flag\n","\n","#Start iterating\n","while stop == False:\n","    \n","    #Map\n","    components_rdd = components_prep.map(lambda row: (mapper(row, centroids_broadcast), (row, 1)))\n","    #vector -> (cluster, (vector, 1))\n","    \n","    #Reduce\n","    result = components_rdd.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n","    #(cluster_i, (vector_a, 1)), (cluster_i, (vector_b, 1)) -> (cluster_i, (vector_a + vector_b, 1 + 1))\n","    \n","    #Update number of iterations\n","    n += 1\n","    \n","    #Update centroids\n","    new_centroids = update_centroids(result, k)\n","    \n","    #Check distance between centroids\n","    error = error_function(initial_centroids, new_centroids, k)\n","    \n","    #Evaluate stop condition\n","    stop = stop_condition(error, n)\n","    \n","    #If we can't stop, broadcast the new centroids and update the initial ones\n","    if stop == False:\n","        centroids_broadcast = broadcasting(new_centroids)\n","        initial_centroids = centroids_broadcast\n","\n","#-------------------------------------------------------------------------------------------------------\n","\n","components_with_clusters = components_rdd.map(lambda row: (row[1][0], row[0]))"]},{"cell_type":"markdown","id":"bb902ab4-f381-47b1-9371-a57df045cf71","metadata":{"id":"bb902ab4-f381-47b1-9371-a57df045cf71"},"source":["---------\n","<br>\n","\n","Save the final RDD to file:"]},{"cell_type":"code","execution_count":null,"id":"d4b2aa4b-8f54-46ec-852b-00c4c24bb9f7","metadata":{"tags":[],"id":"d4b2aa4b-8f54-46ec-852b-00c4c24bb9f7"},"outputs":[],"source":["#Add back the header\n","header = sc.parallelize(np.array([[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"cluster\"]]))\n","header.take(1)"]},{"cell_type":"code","execution_count":null,"id":"84a537c9-699e-40fc-9ba9-a0e513deb428","metadata":{"tags":[],"id":"84a537c9-699e-40fc-9ba9-a0e513deb428"},"outputs":[],"source":["#Format the lines of components RDD\n","components_to_save = components_with_clusters.map(lambda row: toCSVRow_components(row))"]},{"cell_type":"code","execution_count":null,"id":"4849e782-748d-44a4-a572-5bb1cfa0dd68","metadata":{"tags":[],"id":"4849e782-748d-44a4-a572-5bb1cfa0dd68"},"outputs":[],"source":["#Format the header\n","header_to_save = header.map(lambda row: toCSVRow_header(row))"]},{"cell_type":"code","execution_count":null,"id":"b351b036-b84f-4ecf-a3d0-76ce98bcd8b7","metadata":{"tags":[],"id":"b351b036-b84f-4ecf-a3d0-76ce98bcd8b7"},"outputs":[],"source":["#Concatenate the header and the components RDD\n","df = header_to_save.union(components_to_save)"]},{"cell_type":"code","execution_count":null,"id":"02180d94-f736-4d56-9ed8-dc524ee6a6c3","metadata":{"tags":[],"id":"02180d94-f736-4d56-9ed8-dc524ee6a6c3"},"outputs":[],"source":["#Finally save to S3 bucket our file\n","df.saveAsTextFile('s3://admhw4/mapreduce_kmeans.csv')"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":3},"file_extension":".py","mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}